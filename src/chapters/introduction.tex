Time series forecasting---predicting future values of variables within a
domain---is a largely unsolved problem in complex or chaotic domains such as
weather (e.g. humidity, temperature or wind speed) and economics (e.g. currency
exchange rates or stock prices).  Examining the problem with the latest models
in the field of machine learning---\textsc{lstm}-based \textsc{rnn}s---gives
rise to new hope of deepening our understanding of the problem.

\section{Background and motivation}
Artificial neural networks (\textsc{ann}s) have been applied for several decades
to solve classification (determining what \textit{class} a data point belongs
to) and regression (determining the \textit{value} of a dependent variable as a
function of a data point's coordinates) problems.  The early versions of
\textsc{ann}s could only solve classification problems \citep{rosenblatt1958},
while more recent \textsc{ann} models are also capable of handling regression
problems (CITATION NEEDED).  An \textsc{ann}, expressed in simple terms, is a
virtual model of a biological brain, consisting of neurons, synapses and
dendrites, modelled by the \textsc{ann} through cells with scalars, activation
functions and weights.

The earlier models were unable to predict time series sequences (i.e.
determining what class a data point belongs to with respect to its movement over
time, or determining the value of a dependent variable as a function of the data
point's movement over time), something that was later solved through the
introduction of \textsc{rnn}s \citep*{rumelhart1986}, which can handle datasets
with points that have a temporal component dictating their intradependence and
distribution within the dataset, where each point in the sequence is correlated
to previous (with respect to time) points through several known and unknown
factors.

\textsc{rnn}s have historically had problems with vanishing (or exploding)
gradients---meaning they break down during training and become unable to model
the problem well \citep{pascanu2012}---but this problem was solved through the
introduction of the \textsc{lstm}-based \textsc{rnn}s \citep*{hochreiter1997}.

Regardless of whether the problem is approached as a matter of classification or
regression, the temporal aspect renders algorithms not specifically designed for
the task impotent since one single input dataset could belong to several
different classes depending on where, in time or order, it appears in the data
sequence.  The problem should therefore be examined with algorithms designed
specifically for time series sequences, such as \textsc{lstm}-based
\textsc{rnn}s.

\section{Current research}
Currently, in the field of machine learning, hidden Markov models
(\textsc{hmm}s), dynamic Bayesian networks (\textsc{dbn}s), tapped-delay neural
networks (\textsc{tdnn}s) and \textsc{rnn}s are commonly used to handle
sequential datasets, and have been applied with some degree of success to
financial markets \citep{saad1998,kita2012,zhang2004}.

Although much research is currently being done into the application of
\textsc{lstm}-based \textsc{rnn}s in financial markets, the research is still in
its infancy, and although the \textsc{lstm}-based \textsc{rnn} has almost twenty
years on its neck, it is just now that progress is being made to a significant
degree in practice.  The volume of data created in the past couple of
years---about 90\% of all data ever created by humans
\citep*{devakunchari2014}---together with relatively cheap but very fast
graphics processing units (\textsc{gpu}s), have opened up the field of machine
learning to more cost efficient research.

More recently, deep \textsc{lstm}-based \textsc{rnn}s have been applied with
some level of success in predicting future time series sequences from
historical, sequential datasets; such as predicting the weather for the next
twenty-four hours \citep*{zaytar2016}, or predicting the next sequence of words
in natural language \citep*{quoc2014}.

While natural language has obvious structure (\textit{grammar}), weather,
instead, is seemlingy chaotic, controlled by many, many unknown factors.
Despite this; considering the success in weather prediction, \textsc{lstm}-based
\textsc{rnn}s may have the ability to take into account these factors,
especially given an appropriate set of \textit{features}.

\section{Problem statement}
Financial time series are used ubiquitously in algorithmic trading.  In
algorithmic trading, it is imperative that accurate predictions are made about
numerous variables (such as volatility and returns) in order to time market
entry and exit.

Since deep \textsc{lstm}-based \textsc{rnn}s have only been applied within the
algorithmic trading domain to a minimal extent, and since they have shown
success in solving similar problems in other domains, it raises the question
whether the technique can be used to predict a future sequence of financial
variables that can be used to time both entry and exit positions within a
certain time horizon.

Presuming that correlations exist along the temporal dimension of the dataset,
the problem is reduced to a matter of finding an appropriate set of
features enhancing the correlations, on which to train the
\textsc{lstm}-based \textsc{rnn}.  Expressed in a more concise manner, we
attempt to answer the question:

Can financial markets be predicted accurately through the application of
\textsc{lstm}-based \textsc{rnn}s?
